#!/usr/bin/env python3
"""
Silver Pipeline - Transform t·ª´ Bronze v√† l∆∞u v√†o MinIO (Silver Layer) - PySpark + Delta Lake
"""

import os
import re
import json
import boto3
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, udf, lit
from pyspark.sql.types import DoubleType, IntegerType
from delta import configure_spark_with_delta_pip

# ==================== CONFIG ====================
MINIO_ENDPOINT = "http://minio:9000"
ACCESS_KEY = "minio"
SECRET_KEY = "minio123"
BUCKET = "lakehouse"
BRONZE_PREFIX = "bronze/"
PROCESSED_PREFIX = "bronze/processed/"
SILVER_PREFIX = "silver/"
PROCESS_ALL = True

# ==================== MinIO Client ====================
s3 = boto3.client(
    "s3",
    endpoint_url=MINIO_ENDPOINT,
    aws_access_key_id=ACCESS_KEY,
    aws_secret_access_key=SECRET_KEY,
)

# Auto-create bucket n·∫øu ch∆∞a c√≥
existing_buckets = [b["Name"] for b in s3.list_buckets()["Buckets"]]
if BUCKET not in existing_buckets:
    s3.create_bucket(Bucket=BUCKET)
    print(f"ü™£ Created bucket: {BUCKET}")

# ==================== HELPERS ====================
def parse_area(value):
    if not value:
        return None
    try:
        nums = re.findall(r'[\d,.]+', str(value))
        return float(nums[0].replace(",", "")) if nums else None
    except:
        return None


def parse_number(value):
    if not value:
        return None
    try:
        return int(float(str(value)))
    except:
        return None


def normalize_price(value):
    if not value:
        return None
    s = str(value).lower()
    try:
        if "t·ª∑" in s:
            nums = re.findall(r'[\d.]+', s)
            return float(nums[0]) if nums else None
        elif "tri·ªáu" in s:
            nums = re.findall(r'[\d.]+', s)
            return float(nums[0]) / 1000 if nums else None
        else:
            price_str = re.sub(r"[^\d]", "", str(value))
            return int(price_str) / 1e9 if price_str else None
    except:
        return None


def clean_column_names(df):
    """Chu·∫©n h√≥a t√™n c·ªôt cho h·ª£p l·ªá v·ªõi Delta Lake"""
    for old_name in df.columns:
        new_name = (
            old_name.strip()
            .replace(" ", "_")
            .replace("(", "")
            .replace(")", "")
            .replace(";", "")
            .replace(",", "")
            .replace("=", "")
            .replace("/", "_")
        )
        df = df.withColumnRenamed(old_name, new_name)
    return df


# Register UDFs
from pyspark.sql.functions import udf
parse_area_udf = udf(parse_area, DoubleType())
parse_number_udf = udf(parse_number, IntegerType())
normalize_price_udf = udf(normalize_price, DoubleType())

# ==================== SPARK SESSION ====================
builder = (
    SparkSession.builder
    .appName("SilverPipeline")
    .config("spark.hadoop.fs.s3a.endpoint", MINIO_ENDPOINT)
    .config("spark.hadoop.fs.s3a.access.key", ACCESS_KEY)
    .config("spark.hadoop.fs.s3a.secret.key", SECRET_KEY)
    .config("spark.hadoop.fs.s3a.path.style.access", "true")
    .config("spark.hadoop.fs.s3a.connection.ssl.enabled", "false")
)

# ==================== MAIN ====================
def run_silver():
    spark = configure_spark_with_delta_pip(builder).getOrCreate()
    spark.sparkContext.setLogLevel("WARN")
    print("üöÄ Spark session started with Delta Lake")

    # L·∫•y danh s√°ch file trong Bronze
    resp = s3.list_objects_v2(Bucket=BUCKET, Prefix=BRONZE_PREFIX)
    objs = resp.get("Contents", []) if resp else []
    objs = [
        o for o in objs
        if not o["Key"].startswith(PROCESSED_PREFIX) and o["Key"].endswith(".json")
    ]

    if not objs:
        print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu Bronze ch∆∞a x·ª≠ l√Ω.")
        return

    to_process = sorted(objs, key=lambda x: x["LastModified"])
    if not PROCESS_ALL:
        to_process = [to_process[-1]]

    for obj in to_process:
        key = obj["Key"]
        print(f"üîÑ X·ª≠ l√Ω file: {key}")

        # ƒê·ªçc JSON t·ª´ MinIO
        raw_bytes = s3.get_object(Bucket=BUCKET, Key=key)["Body"].read()
        try:
            data = json.loads(raw_bytes.decode("utf-8"))
            if isinstance(data, dict):
                data = [data]
        except Exception as e:
            print(f"‚ùå Kh√¥ng parse ƒë∆∞·ª£c JSON t·ª´ {key}: {e}")
            move_to_processed(key)
            continue

        if not data:
            print(f"‚ö†Ô∏è File r·ªóng: {key}")
            move_to_processed(key)
            continue

        # Load v√†o Spark DataFrame
        df = spark.read.json(spark.sparkContext.parallelize([json.dumps(r) for r in data]))
        print(f"üì¶ S·ªë record raw: {df.count()}")

        # Chu·∫©n h√≥a schema
        df_standard = df.select(
            col("address").alias("Address"),
            col("Di·ªán t√≠ch ƒë·∫•t").alias("Area"),
            col("Chi·ªÅu ngang").alias("Frontage"),
            col("ƒê·∫∑c ƒëi·ªÉm nh√†/ƒë·∫•t").alias("Access_Road"),
            col("H∆∞·ªõng c·ª≠a ch√≠nh").alias("House_Direction"),
            col("T·ªïng s·ªë t·∫ßng").alias("Floors"),
            col("S·ªë ph√≤ng ng·ªß").alias("Bedrooms"),
            col("S·ªë ph√≤ng v·ªá sinh").alias("Bathrooms"),
            col("Gi·∫•y t·ªù ph√°p l√Ω").alias("Legal_Status"),
            col("T√¨nh tr·∫°ng n·ªôi th·∫•t").alias("Furniture_State"),
            col("price").alias("Price")
        )

        df_standard = clean_column_names(df_standard)
        print(f"‚úÖ Sau transform: {df_standard.count()} records")

        # L·∫•y timestamp t·ª´ t√™n file bronze
        filename = os.path.basename(key)
        timestamp = filename.split("_")[1]  # crawl_YYYYMMDD_HHMMSS.json
        date_fmt = f"{timestamp[:4]}-{timestamp[4:6]}-{timestamp[6:]}"  # YYYY-MM-DD

        # T·∫°o key d·∫°ng partitioned
        silver_key = (
            key.replace(BRONZE_PREFIX, f"{SILVER_PREFIX}/date={date_fmt}/")
            .replace(".json", "")
            .replace("crawl_", "crawl_cleaned_")
        )
        silver_path = f"s3a://{BUCKET}/{silver_key}"

        # Th√™m c·ªôt Date
        df_standard = df_standard.withColumn("Date", lit(date_fmt))

        # Ghi Delta Table v√†o MinIO
        df_standard.write.format("delta").mode("overwrite").partitionBy("Date").save(silver_path)
        print(f"üíæ ƒê√£ l∆∞u Silver (Delta Lake) -> {silver_path}")

        # Move file ƒë√£ x·ª≠ l√Ω
        move_to_processed(key)


def move_to_processed(key):
    """Di chuy·ªÉn file t·ª´ bronze/ sang bronze/processed/"""
    try:
        processed_key = key.replace(BRONZE_PREFIX, PROCESSED_PREFIX)
        s3.copy_object(Bucket=BUCKET, CopySource={"Bucket": BUCKET, "Key": key}, Key=processed_key)
        s3.delete_object(Bucket=BUCKET, Key=key)
        print(f"üì¶ ƒê√£ move {key} -> {processed_key}")
    except Exception as e:
        print(f"‚ö†Ô∏è L·ªói khi move_to_processed: {e}")


if __name__ == "__main__":
    run_silver()
