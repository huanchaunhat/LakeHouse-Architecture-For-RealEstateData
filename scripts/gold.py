#!/usr/bin/env python3
"""
üèÜ Gold Pipeline - t·ª´ Silver (Parquet) sang Gold (Delta)
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, to_date, avg, count, sum as _sum, when, lit, input_file_name, regexp_extract
from delta import configure_spark_with_delta_pip
from datetime import datetime

# =====================================================
# 1Ô∏è‚É£ Kh·ªüi t·∫°o SparkSession (v·ªõi Delta Lake)
# =====================================================
builder = (
    SparkSession.builder.appName("GoldPipeline")
    .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension")
    .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog")
)

spark = configure_spark_with_delta_pip(builder).getOrCreate()

# =====================================================
# 2Ô∏è‚É£ ƒê·ªçc d·ªØ li·ªáu t·ª´ Silver
# =====================================================
silver_path = "s3a://lakehouse/silver/date=*/crawl_cleaned_*/Date=*/"
print("üìÇ ƒê·ªçc d·ªØ li·ªáu Silver t·ª´:", silver_path)

df_silver = (
    spark.read.format("parquet")
    .option("recursiveFileLookup", "true")
    .load(silver_path)
)

print("‚úÖ D·ªØ li·ªáu Silver m·∫´u:")
df_silver.show(5)

# =====================================================
# 3Ô∏è‚É£ Chu·∫©n h√≥a c·ªôt Date (n·∫øu thi·∫øu)
# =====================================================
# Th√™m c·ªôt Date t·ª´ ƒë∆∞·ªùng d·∫´n file n·∫øu ch∆∞a c√≥
if "Date" not in df_silver.columns:
    df_silver = (
        df_silver.withColumn("file_path", input_file_name())
        .withColumn("Date", regexp_extract(col("file_path"), r"Date=(\d{4}-\d{2}-\d{2})", 1))
        .drop("file_path")
    )

# N·∫øu v·∫´n ch∆∞a c√≥ Date, g√°n theo ng√†y hi·ªán t·∫°i
if "Date" not in df_silver.columns or df_silver.filter(col("Date") == "").count() > 0:
    today = datetime.now().strftime("%Y-%m-%d")
    df_silver = df_silver.withColumn("Date", lit(today))

# =====================================================
# 4Ô∏è‚É£ L√†m s·∫°ch & chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu
# =====================================================
if "amount" in df_silver.columns:
    df_gold = (
        df_silver.groupBy("Date")
        .agg(
            count("*").alias("num_records"),
            _sum("amount").alias("total_amount"),
            avg("amount").alias("avg_amount")
        )
        .withColumn("etl_status", lit("success"))
    )
elif "Price" in df_silver.columns:
    df_gold = (
        df_silver.groupBy("Date")
        .agg(
            count("*").alias("num_records"),
            _sum("Price").alias("total_price"),
            avg("Price").alias("avg_price")
        )
        .withColumn("etl_status", lit("success"))
    )
else:
    df_gold = df_silver.withColumn("etl_status", lit("cleaned"))

print("‚úÖ D·ªØ li·ªáu Gold sau x·ª≠ l√Ω:")
df_gold.show(5)

# =====================================================
# 5Ô∏è‚É£ Ghi d·ªØ li·ªáu ra t·∫ßng Gold (Delta)
# =====================================================
gold_path = "s3a://lakehouse/gold/date_partitioned"
(
    df_gold.write.format("delta")
    .mode("overwrite")
    .partitionBy("Date")
    .save(gold_path)
)

print("üèÅ ƒê√£ ghi d·ªØ li·ªáu ra t·∫ßng Gold:", gold_path)

# =====================================================
# 6Ô∏è‚É£ (T√πy ch·ªçn) T·∫°o b·∫£ng Gold trong Spark SQL
# =====================================================
spark.sql(f"""
    CREATE TABLE IF NOT EXISTS gold_table
    USING DELTA
    LOCATION '{gold_path}'
""")

print("üìä B·∫£ng gold_table ƒë√£ s·∫µn s√†ng!")

spark.stop()
