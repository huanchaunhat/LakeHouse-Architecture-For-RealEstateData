#!/usr/bin/env python3
"""
Silver Pipeline - Transform & Clean t·ª´ Bronze v√† l∆∞u v√†o MinIO (Silver Layer) - PySpark + Delta Lake
"""

import os
import re
import json
import boto3
from pyspark.sql import SparkSession
from pyspark.sql.functions import (
    col, udf, lit, trim, initcap, when, concat_ws, round as spark_round
)
from pyspark.sql.types import DoubleType, IntegerType
from delta import configure_spark_with_delta_pip

# ==================== CONFIG ====================
MINIO_ENDPOINT = "http://minio:9000"
ACCESS_KEY = "minio"
SECRET_KEY = "minio123"
BUCKET = "lakehouse"
BRONZE_PREFIX = "bronze/"
PROCESSED_PREFIX = "bronze/processed/"
SILVER_PREFIX = "silver/"
PROCESS_ALL = True

# ==================== MinIO Client ====================
s3 = boto3.client(
    "s3",
    endpoint_url=MINIO_ENDPOINT,
    aws_access_key_id=ACCESS_KEY,
    aws_secret_access_key=SECRET_KEY,
)

if BUCKET not in [b["Name"] for b in s3.list_buckets()["Buckets"]]:
    s3.create_bucket(Bucket=BUCKET)
    print(f"ü™£ Created bucket: {BUCKET}")

# ==================== HELPERS ====================
def parse_area(v):
    if not v:
        return None
    try:
        nums = re.findall(r'[\d,.]+', str(v))
        return float(nums[0].replace(",", "")) if nums else None
    except:
        return None

def parse_number(v):
    if not v:
        return None
    try:
        return int(float(str(v)))
    except:
        return None

def normalize_price(v):
    if not v:
        return None
    s = str(v).lower()
    try:
        if "t·ª∑" in s:
            nums = re.findall(r'[\d,.]+', s)
            return float(nums[0].replace(",", ".")) if nums else None
        elif "tri·ªáu" in s:
            nums = re.findall(r'[\d,.]+', s)
            return float(nums[0].replace(",", ".")) / 1000 if nums else None
        else:
            nums = re.findall(r'[\d.]+', s)
            return float(nums[0]) if nums else None
    except:
        return None

def clean_column_names(df):
    for old in df.columns:
        new = (
            old.strip()
            .replace(" ", "_")
            .replace("(", "")
            .replace(")", "")
            .replace(";", "")
            .replace(",", "")
            .replace("=", "")
            .replace("/", "_")
        )
        df = df.withColumnRenamed(old, new)
    return df

# Register UDFs
parse_area_udf = udf(parse_area, DoubleType())
parse_number_udf = udf(parse_number, IntegerType())
normalize_price_udf = udf(normalize_price, DoubleType())

# ==================== SPARK ====================
builder = (
    SparkSession.builder
    .appName("SilverPipeline-Delta")
    .config("spark.hadoop.fs.s3a.endpoint", MINIO_ENDPOINT)
    .config("spark.hadoop.fs.s3a.access.key", ACCESS_KEY)
    .config("spark.hadoop.fs.s3a.secret.key", SECRET_KEY)
    .config("spark.hadoop.fs.s3a.path.style.access", "true")
    .config("spark.hadoop.fs.s3a.connection.ssl.enabled", "false")
)

spark = configure_spark_with_delta_pip(builder).getOrCreate()
spark.sparkContext.setLogLevel("WARN")
print("üöÄ Spark session started with Delta Lake")

# ==================== MAIN ====================
def run_silver():
    resp = s3.list_objects_v2(Bucket=BUCKET, Prefix=BRONZE_PREFIX)
    objs = resp.get("Contents", []) if resp else []
    objs = [o for o in objs if not o["Key"].startswith(PROCESSED_PREFIX) and o["Key"].endswith(".json")]
    if not objs:
        print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu Bronze ch∆∞a x·ª≠ l√Ω.")
        return

    to_process = sorted(objs, key=lambda x: x["LastModified"])
    if not PROCESS_ALL:
        to_process = [to_process[-1]]

    for obj in to_process:
        key = obj["Key"]
        print(f"\nüîÑ ƒêang x·ª≠ l√Ω file: {key}")

        raw_bytes = s3.get_object(Bucket=BUCKET, Key=key)["Body"].read()
        try:
            data = json.loads(raw_bytes.decode("utf-8"))
            if isinstance(data, dict):
                data = [data]
        except Exception as e:
            print(f"‚ùå Parse JSON l·ªói: {e}")
            move_to_processed(key)
            continue

        if not data:
            print(f"‚ö†Ô∏è File r·ªóng: {key}")
            move_to_processed(key)
            continue

        df = spark.read.json(spark.sparkContext.parallelize([json.dumps(r) for r in data]))
        print(f"üì¶ Raw records: {df.count()}")

        df = clean_column_names(df)

        # üß≠ Chu·∫©n h√≥a c·ªôt Address
        df = df.withColumn(
            "Address",
            when(col("address").isNotNull(), trim(col("address")))
            .otherwise(
                when(col("ƒê·ªãa_ch·ªâ").isNotNull(), trim(col("ƒê·ªãa_ch·ªâ")))
                .otherwise(
                    concat_ws(", ",
                        col("Ph∆∞·ªùng,_th·ªã_x√£,_th·ªã_tr·∫•n"),
                        col("Qu·∫≠n,_Huy·ªán"),
                        col("T·ªânh,_th√†nh_ph·ªë")
                    )
                )
            )
        )

        # === L√†m s·∫°ch d·ªØ li·ªáu ===
        df_clean = (
            df
            .withColumn("Area", parse_area_udf(col("Di·ªán_t√≠ch_ƒë·∫•t")))
            .withColumn("Frontage", parse_area_udf(col("Chi·ªÅu_ngang")))
            .withColumn("Floors", parse_number_udf(col("T·ªïng_s·ªë_t·∫ßng")))
            .withColumn("Bedrooms", parse_number_udf(col("S·ªë_ph√≤ng_ng·ªß")))
            .withColumn("Bathrooms", parse_number_udf(col("S·ªë_ph√≤ng_v·ªá_sinh")))
            .withColumn("Price", normalize_price_udf(col("price")))
            .withColumn("Address", initcap(trim(col("Address"))))
            .withColumn("Legal_Status", initcap(trim(col("Gi·∫•y_t·ªù_ph√°p_l√Ω"))))
            .withColumn("House_Direction", initcap(trim(col("H∆∞·ªõng_c·ª≠a_ch√≠nh"))))
            .filter(col("Area").isNotNull() & (col("Area") > 0))
            .filter(col("Price").isNotNull() & (col("Price") > 0))
            .withColumn("Price_per_m2", spark_round(col("Price") / col("Area"), 3))
        )

        cleaned_count = df_clean.count()
        print(f"‚úÖ Sau khi l√†m s·∫°ch: {cleaned_count} b·∫£n ghi h·ª£p l·ªá")

        # === Ghi ra Silver Layer (Delta Lake) ===
        filename = os.path.basename(key)
        timestamp = filename.split("_")[1] if "_" in filename else "unknown"
        date_fmt = f"{timestamp[:4]}-{timestamp[4:6]}-{timestamp[6:]}" if len(timestamp) == 8 else "unknown"
        silver_key = (
            key.replace(BRONZE_PREFIX, f"{SILVER_PREFIX}/date={date_fmt}/")
            .replace(".json", "")
            .replace("crawl_", "crawl_cleaned_")
        )
        silver_path = f"s3a://{BUCKET}/{silver_key}"

        df_clean = df_clean.withColumn("Date", lit(date_fmt))
        df_clean.write.format("delta").mode("overwrite").partitionBy("Date").save(silver_path)
        print(f"üíæ ƒê√£ l∆∞u Silver (Delta Lake) t·∫°i: {silver_path}")

        move_to_processed(key)

def move_to_processed(key):
    try:
        processed_key = key.replace(BRONZE_PREFIX, PROCESSED_PREFIX)
        s3.copy_object(Bucket=BUCKET, CopySource={"Bucket": BUCKET, "Key": key}, Key=processed_key)
        s3.delete_object(Bucket=BUCKET, Key=key)
        print(f"üì¶ ƒê√£ move {key} -> {processed_key}")
    except Exception as e:
        print(f"‚ö†Ô∏è L·ªói khi move_to_processed: {e}")

if __name__ == "__main__":
    run_silver()


# scripts/load_bronze_to_table.py
from pyspark.sql import SparkSession
from delta import configure_spark_with_delta_pip

# ==================== SPARK ====================
builder = (
    SparkSession.builder
    .appName("SilverPipeline-Delta")
    .config("spark.hadoop.fs.s3a.endpoint", MINIO_ENDPOINT)
    .config("spark.hadoop.fs.s3a.access.key", ACCESS_KEY)
    .config("spark.hadoop.fs.s3a.secret.key", SECRET_KEY)
    .config("spark.hadoop.fs.s3a.path.style.access", "true")
    .config("spark.hadoop.fs.s3a.connection.ssl.enabled", "false")
)

spark = configure_spark_with_delta_pip(builder).getOrCreate()
spark.sparkContext.setLogLevel("WARN")
print("üöÄ Spark session started with Delta Lake")

BRONZE_PATH = f"s3a://{BUCKET}/bronze/*.jsonl"
BRONZE_TABLE_NAME = "bronze.raw_properties" # T√™n b·∫£ng th√¥
CHECKPOINT_PATH = f"s3a://{BUCKET}/_checkpoints/bronze_raw"

def load_bronze_to_table():
    df_raw = spark.readStream.format("json") \
        .option("spark.sql.streaming.schemaInference", "true") \
        .load(BRONZE_PATH)

    query = (
        df_raw.writeStream
        .format("delta")
        .outputMode("append")
        .option("checkpointLocation", CHECKPOINT_PATH)
        .trigger(once=True)
        .toTable(BRONZE_TABLE_NAME) # Ghi th·∫≥ng ra 1 b·∫£ng Delta
    )
    query.awaitTermination()
    print(f"‚úÖ ƒê√£ load d·ªØ li·ªáu th√¥ v√†o b·∫£ng {BRONZE_TABLE_NAME}")

if __name__ == "__main__":
    load_bronze_to_table()