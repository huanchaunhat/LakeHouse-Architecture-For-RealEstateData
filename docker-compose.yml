services:
  mysql:
    image: mysql:5.7
    container_name: mysql
    volumes:
      - ./docker-volumes/mysql:/var/lib/mysql
    ports:
      - "3308:3306"
    env_file:
      - .env
    networks:
      - data_network

  minio:
    hostname: minio
    image: "minio/minio"
    container_name: minio
    ports:
      - "9001:9001"
      - "9000:9000"
    command: ["server", "/data", "--console-address", ":9001"]
    volumes:
      - ./docker-volumes/minio/data:/data
    env_file:
      - .env
    networks:
      - data_network

  hive-metastore:
    container_name: hive-metastore
    hostname: hive-metastore
    image: "bitsondatadev/hive-metastore"
    entrypoint: /entrypoint.sh
    ports:
      - "9083:9083"
    volumes:
      - ./hive-metastore/metastore-site.xml:/opt/apache-hive-metastore-3.0.0-bin/conf/metastore-site.xml:ro
    environment:
      METASTORE_DB_HOSTNAME: mysql
    networks:
      - data_network
    depends_on:
      - mysql
      - minio
    healthcheck:
      test: ["CMD-SHELL", "nc -z localhost 9083"]
      interval: 30s
      timeout: 10s
      retries: 5

  spark-master:
    build:
      context: ./spark
      dockerfile: ./Dockerfile
    container_name: "spark-master"
    ports:
      - "7077:7077"
      - "8081:8080"
    healthcheck:
      test: ["CMD-SHELL", "nc -z localhost 7077"]
      interval: 10s
      timeout: 5s
      retries: 5
    environment:
      - SPARK_MODE=master
      - SPARK_MASTER_HOST=0.0.0.0
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_USER=spark
    volumes:
      - ./spark/spark-defaults.conf:/opt/bitnami/spark/conf/spark-defaults.conf
      - ./scripts:/opt/bitnami/spark/scripts
    networks:
      - data_network

  spark-worker-1:
    build:
      context: ./spark
      dockerfile: ./Dockerfile
    container_name: "spark-worker-1"
    env_file:
      - .env
    depends_on:
      - spark-master
    networks:
      - data_network

  spark-worker-2:
    build:
      context: ./spark
      dockerfile: ./Dockerfile
    container_name: "spark-worker-2"
    env_file:
      - .env
    depends_on:
      - spark-master
    networks:
      - data_network

  spark-thrift-server:
    build:
      context: ./spark
      dockerfile: ./Dockerfile
    container_name: "spark-thrift-server"
    restart: always
    depends_on:
      spark-master:
        condition: service_started
      hive-metastore:
        condition: service_healthy
    ports:
      - "4040:4040"
      - "10000:10000"
    # Giới hạn CPU/Memory cho container
    deploy:
      resources:
        limits:
          cpus: "1.5"
          memory: 2G
        reservations:
          cpus: "0.5"
          memory: 512M
    command: sh -c "
      sleep 10 && ./sbin/start-thriftserver.sh \
      --driver-java-options '-Dhive.metastore.uris=thrift://hive-metastore:9083' \
      --master spark://spark-master:7077 \
      --driver-cores 1 \
      --driver-memory 1g \
      --total-executor-cores 1 \
      --executor-cores 1 \
      --executor-memory 1g \
      --conf spark.dynamicAllocation.enabled=false \
      --conf spark.cores.max=1 \
      --conf spark.sql.shuffle.partitions=8 \
      --conf spark.default.parallelism=4"
    volumes:
      - ./spark/spark-defaults.conf:/opt/bitnami/spark/conf/spark-defaults.conf
      - ./spark/hive-site.xml:/opt/bitnami/spark/conf/hive-site.xml
    networks:
      - data_network

  superset:
    image: apache/superset:3.0.0
    container_name: superset
    ports:
      - "8089:8088" # Changed to 8089 externally to avoid conflict with Airflow
    environment:
      - SUPERSET_SECRET_KEY=ThisIsASecretKeyForSupersetPleaseChangeInProduction
      - SUPERSET_LOAD_EXAMPLES=yes
    volumes:
      - ./superset:/app/superset_home
    networks:
      - data_network
    command: >
      sh -c "pip install pyhive thrift sasl thrift_sasl &&
             superset db upgrade &&
             superset init &&
             gunicorn --bind 0.0.0.0:8088 --timeout 120 --workers 4 'superset.app:create_app()'"

  airflow-webserver:
    build: ./airflow
    container_name: airflow-webserver
    restart: always
    depends_on:
      mysql-airflow:
        condition: service_started
      spark-master:
        condition: service_healthy
    env_file:
      - .env
    environment:
      - LOAD_EX=n
      - AIRFLOW__WEBSERVER__RBAC=False
      - AIRFLOW__WEBSERVER__SESSION_BACKEND=securecookie
      - EXECUTOR=Local
    volumes:
      - ./real_estate_data:/usr/local/airflow/real_estate_data
      - ./airflow/dags:/usr/local/airflow/dags
      - ./airflow/logs:/usr/local/airflow/logs
      - ./airflow/plugins:/usr/local/airflow/plugins
      - ./airflow/config/airflow.cfg:/usr/local/airflow/config/airflow.cfg
      - ./airflow/jars:/usr/local/airflow/jars
      - ./scripts:/usr/local/airflow/scripts
      - /var/run/docker.sock:/var/run/docker.sock
      - ./dbt:/opt/airflow/dbt
    ports:
      - "8088:8088"
      - "8888:8888"
    command: webserver
    healthcheck:
      test: ["CMD-SHELL", "[ -f /usr/local/airflow/airflow-webserver.pid ]"]
      interval: 30s
      timeout: 30s
      retries: 3
    networks:
      - data_network

  airflow-scheduler:
    build: ./airflow
    container_name: airflow-scheduler
    restart: always
    depends_on:
      mysql-airflow:
        condition: service_started
      spark-master:
        condition: service_healthy
    env_file:
      - .env
    environment:
      - LOAD_EX=n
      - AIRFLOW__WEBSERVER__RBAC=False
      - AIRFLOW__WEBSERVER__SESSION_BACKEND=securecookie
      - EXECUTOR=Local
    volumes:
      - ./real_estate_data:/usr/local/airflow/real_estate_data
      - ./airflow/dags:/usr/local/airflow/dags
      - ./airflow/logs:/usr/local/airflow/logs
      - ./airflow/plugins:/usr/local/airflow/plugins
      - ./airflow/config/airflow.cfg:/usr/local/airflow/config/airflow.cfg
      - ./airflow/jars:/usr/local/airflow/jars
      - ./scripts:/usr/local/airflow/scripts
      - /var/run/docker.sock:/var/run/docker.sock
      - ./dbt:/opt/airflow/dbt
    command: scheduler
    networks:
      - data_network

  mysql-airflow:
    image: mysql:5.7
    container_name: mysql-airflow
    ports:
      - "3307:3307"
    environment:
      - MYSQL_ROOT_PASSWORD=${AIRFLOW_MYSQL_ROOT_PASSWORD}
      - MYSQL_DATABASE=${AIRFLOW_MYSQL_DATABASE}
      - MYSQL_USER=${AIRFLOW_MYSQL_USER}
      - MYSQL_PASSWORD=${AIRFLOW_MYSQL_PASSWORD}
      - MYSQL_TCP_PORT=${AIRFLOW_MYSQL_TCP_PORT}
    expose:
      - "3307"
    command: --default-authentication-plugin=mysql_native_password
    volumes:
      - ./mysql/my.cnf:/etc/mysql/my.cnf
      - ./docker-volumes/mysql-airflow:/var/lib/mysql #Lưu trữ dữ liệu MySQL của Airflow
    networks:
      - data_network

  # mlflow:
  #   build:
  #     context: ./mlflow
  #     dockerfile: Dockerfile
  #   container_name: mlflow
  #   ports:
  #     - "5000:5000"
  #   env_file:
  #     - .env
  #   environment:
  #     - PORT=5000
  #     - FILE_DIR=/mlflow
  #     - AWS_BUCKET=mlflow
  #     - PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python
  #   networks:
  #     - data_network

  # model_serving:
  #   build:
  #     context: ./model_server
  #     dockerfile: Dockerfile
  #   container_name: recommender_model
  #   ports:
  #     - "5001:5001"
  #   env_file:
  #     - .env
  #   volumes:
  #     - ./model_server/main.py:/opt/mlflow/main.py
  #   environment:
  #     - AWS_BUCKET=mlflow
  #     - FILE_DIR=/mlflow
  #   networks:
  #     - data_network

  # app:
  #   build:
  #     context: ./app
  #     dockerfile: Dockerfile
  #   container_name: app
  #   ports:
  #     - "8051:8051"
  #   command: bash -c "export PYTHONPATH=/home/app"
  #   volumes:
  #     - ./app:/home/app
  #   networks:
  #     - data_network

networks:
  data_network:
    driver: bridge
    name: data_network
